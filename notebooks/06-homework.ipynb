{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9fea19-b68d-4a26-97e8-9a5c740e9de3",
   "metadata": {},
   "source": [
    "1.Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc5e7-bc8c-4c50-bc46-86075c6775ff",
   "metadata": {},
   "source": [
    "Импортируем библиотеки, загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e7ba85-a97f-4636-8658-f5dfe142ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e73482-8ed9-4ff6-a73e-9c9100d2a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>helpful_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-12T14:41:14-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-11T19:39:13-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>The service on Singapore Airlines Suites Class...</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-11T12:20:23-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "      <td>Don’t give them your money</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-11T07:12:27-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "      <td>Best Airline in the World</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-10T05:34:18-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              published_date published_platform  rating    type  \\\n",
       "0  2024-03-12T14:41:14-04:00            Desktop       3  review   \n",
       "1  2024-03-11T19:39:13-04:00            Desktop       5  review   \n",
       "2  2024-03-11T12:20:23-04:00            Desktop       1  review   \n",
       "3  2024-03-11T07:12:27-04:00            Desktop       5  review   \n",
       "4  2024-03-10T05:34:18-04:00            Desktop       2  review   \n",
       "\n",
       "                                                text  \\\n",
       "0  We used this airline to go from Singapore to L...   \n",
       "1  The service on Singapore Airlines Suites Class...   \n",
       "2  Booked, paid and received email confirmation f...   \n",
       "3  Best airline in the world, seats, food, servic...   \n",
       "4  Premium Economy Seating on Singapore Airlines ...   \n",
       "\n",
       "                                               title  helpful_votes  \n",
       "0                                                 Ok              0  \n",
       "1  The service in Suites Class makes one feel lik...              0  \n",
       "2                         Don’t give them your money              0  \n",
       "3                          Best Airline in the World              0  \n",
       "4  Premium Economy Seating on Singapore Airlines ...              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('archive (4)/singapore_airlines_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539565f-ff29-42af-a48d-8ae7829f0589",
   "metadata": {},
   "source": [
    "Уберем пунктуацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c031b4e-70cb-4dbd-b115-746d36143b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    punctuation = [\".\", \",\", \"!\", \"?\", \":\", \";\", \"-\", \"(\", \")\"]\n",
    "    lst = text.split()\n",
    "    fixed_text = []\n",
    "    for word in lst:\n",
    "        for Mark in punctuation:\n",
    "            word = word.replace(Mark, \"\")\n",
    "        fixed_text.append(word.lower())\n",
    "    s = \" \".join(fixed_text)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c679da06-01ad-426b-a604-6191ced34309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       we used this airline to go from singapore to l...\n",
       "1       the service on singapore airlines suites class...\n",
       "2       booked paid and received email confirmation fo...\n",
       "3       best airline in the world seats food service a...\n",
       "4       premium economy seating on singapore airlines ...\n",
       "                              ...                        \n",
       "9995    first part done with singapore airlines  accep...\n",
       "9996    and again a great flight with singapore air gr...\n",
       "9997    we flew business class from frankfurt via sing...\n",
       "9998    as always the a380 aircraft was spotlessly pre...\n",
       "9999    as always singapore airlines has done it again...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = df[\"text\"].apply(preprocess_text)\n",
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0ab17-4617-4036-8744-06db4cd47fc1",
   "metadata": {},
   "source": [
    "Преобразуем в TF-IDF, обучим логистическую регрессию, будем использовать метрику f1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a432020-5789-4c2f-b775-ac8822f0c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7176930818076696\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning) #чтобы не всплывал ConvergenceWarning\n",
    "vectorizer = TfidfVectorizer()\n",
    "new_train = vectorizer.fit_transform(new_text)\n",
    "model = LogisticRegression()\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_train, df[\"rating\"], train_size=0.8)\n",
    "model.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(f1_score(prediction, y_test, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33c4f0-63c8-443a-af7b-a40db304ac2e",
   "metadata": {},
   "source": [
    "2.Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b51284-bfd6-41b3-a023-1cbd082bc1c2",
   "metadata": {},
   "source": [
    "Удалим стоп-слова и применим лемматизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071cae46-9937-4362-a8ca-1e4c90000587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/banana_cat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def lemmatization_and_del_stopw(text):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    lst = preprocessed_text.split()\n",
    "    filtered_and_lemmatized_lst = [lemmatizer.lemmatize(word) for word in lst if word not in stop_words]\n",
    "    filtered_text = \" \".join(filtered_and_lemmatized_lst)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e30b863-2dec-4e31-9777-89c0ec528021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.7075165438769789\n",
      "DecisionTreeClassifier: 0.5343393731090034\n",
      "SVM(RBF): 0.7219223442218528\n"
     ]
    }
   ],
   "source": [
    "new_text = df[\"text\"].apply(lemmatization_and_del_stopw)\n",
    "vectorizer = TfidfVectorizer()\n",
    "new_train = vectorizer.fit_transform(new_text)\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_train, df[\"rating\"], train_size=0.8)\n",
    "\n",
    "lrmodel = LogisticRegression()\n",
    "lrmodel.fit(x_train, y_train)\n",
    "prediction = lrmodel.predict(x_test)\n",
    "print(\"LogisticRegression:\", f1_score(prediction, y_test, average=\"weighted\"))\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(x_train, y_train)\n",
    "prediction = tree_model.predict(x_test)\n",
    "print(\"DecisionTreeClassifier:\", f1_score(prediction, y_test, average=\"weighted\"))\n",
    "\n",
    "svm_model = SVC(kernel = \"rbf\")\n",
    "svm_model.fit(x_train, y_train)\n",
    "prediction = svm_model.predict(x_test)\n",
    "print(\"SVM(RBF):\", f1_score(prediction, y_test, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b6348-dcef-4f26-922f-483dc095d121",
   "metadata": {},
   "source": [
    "Как видно, лучший результат показала модель SVM на радиальном базисном ядре."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a9c08-e267-46e4-8750-5120bdbff198",
   "metadata": {},
   "source": [
    "Придумаем отзывы, проверим работу модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab73ee8-1f85-46a8-be4c-9db520f6e47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "good_review = '''The service was incredibly slow, and the staff were rude and inattentive. The food was cold and tasted stale.\n",
    "Overall, it was a disappointing experience, and I would not recommend this restaurant to anyone. i will never ever use this fucking airlines\n",
    "'''\n",
    "bad_review  = '''The service at this restaurant was excellent. The staff were friendly and attentive. The food was delicious and beautifully presented.\n",
    "Overall, it was a wonderful dining experience, and I would definitely recommend it to others.'''\n",
    "reviews = [bad_review, good_review]\n",
    "new_df = pd.DataFrame(reviews, columns=['reviews'])\n",
    "text = new_df['reviews'].apply(lemmatization_and_del_stopw)\n",
    "vectorizer = TfidfVectorizer()\n",
    "new_test = vectorizer.fit_transform(new_text)\n",
    "print(svm_model.predict(new_test[0]))\n",
    "print(svm_model.predict(new_test[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
